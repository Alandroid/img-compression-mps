{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, \"/Users/maxge/Documents/Studium/München/02_SS 2024/QEL/Block encoding generalization/img-compression-mps/ND MPS Encoding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maxge/miniconda3/envs/QEL/lib/python3.10/site-packages/cotengra/hyperoptimizers/hyper.py:33: UserWarning: Couldn't import `kahypar` - skipping from default hyper optimizer and using basic `labels` method instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from mps_ND import NDMPS\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "from utils_ND import *\n",
    "import quimb.tensor as qtn\n",
    "from scipy.fftpack import dct, idct\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_factorlist(shape):\n",
    "    # todo rename to actually get_block_sizes\n",
    "    factor_lists = []\n",
    "    for dim_size in shape:\n",
    "        if dim_size == 1:\n",
    "            # Handle dimension of size 1 by assigning a factor of 1\n",
    "            factors = [1]\n",
    "        else:\n",
    "            factors_dict = factorint(dim_size)\n",
    "            # Extract prime factors and repeat them according to their exponents\n",
    "            factors = []\n",
    "            for prime, exponent in sorted(factors_dict.items()):\n",
    "                factors.extend([prime] * exponent)\n",
    "        # Sort the factors in ascending order\n",
    "        factors_sorted = sorted(factors)\n",
    "        factor_lists.append(factors_sorted)\n",
    "    \n",
    "    # Step 2: Balancing the number of factors across all dimensions\n",
    "    # Determine the minimum number of factors among all dimensions\n",
    "    min_factors = min(len(factors) for factors in factor_lists)\n",
    "    \n",
    "    # Balance factors by grouping the smallest factors in dimensions with more factors\n",
    "    for idx, factors in enumerate(factor_lists):\n",
    "        if len(factors) > min_factors:\n",
    "            factor_lists[idx] = balance_factors(factors, min_factors)\n",
    "        elif len(factors) < min_factors:\n",
    "            # If a dimension has fewer factors, pad with 1s to reach min_factors\n",
    "            # This effectively treats missing factors as trivial\n",
    "            factor_lists[idx].extend([1] * (min_factors - len(factors)))\n",
    "            factor_lists[idx] = sorted(factor_lists[idx])\n",
    "        # If equal, do nothing\n",
    "    for idx, list in enumerate(factor_lists):\n",
    "        if idx%2 == 1:\n",
    "            factor_lists[idx] = factor_lists[idx][::-1] \n",
    "\n",
    "    factor_lists = np.array(factor_lists).T\n",
    "    prod_block_sizes = np.ones((len(factor_lists)+1, len(factor_lists[0])), dtype = int)\n",
    "    prod_block_sizes[1:-1] = np.cumprod(factor_lists[-1:0:-1], axis =0)[::-1]\n",
    "    prod_block_sizes[0] = prod_block_sizes[0] * 1e100\n",
    "\n",
    "    return factor_lists, prod_block_sizes\n",
    "\n",
    "@time_function\n",
    "def hierarchical_block_indexing(index, prod_block_sizes):\n",
    "    return np.floor(np.mod(index.reshape([1]+list(index.shape)), prod_block_sizes[:-1].reshape(list(prod_block_sizes[:-1].shape)+[1]*(prod_block_sizes.shape[1])))/prod_block_sizes[1:].reshape(list(prod_block_sizes[1:].shape)+[1]*(prod_block_sizes.shape[1]))).astype(int)\n",
    "\n",
    "def gen_encoding_map(shape):\n",
    "    dim = len(shape)\n",
    "    block_sizes, prod_blocks = get_factorlist(shape)\n",
    "    indices_all = np.indices(shape)\n",
    "    mapped_indexes = hierarchical_block_indexing(indices_all, prod_blocks)\n",
    "    final_map = np.empty([len(block_sizes)]+list(shape))\n",
    "    for i in range(len(block_sizes)):\n",
    "        final_map[i] = np.ravel_multi_index(mapped_indexes[i], block_sizes[i])\n",
    "    return np.prod(block_sizes, axis= 1), final_map.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NDMPS_new:\n",
    "    def __init__(self, mps=None, qubit_size=None, encoding_map=None, norm=True, mode=\"Std\", min_value = 0, max_value = 1):\n",
    "        self.qubit_size = qubit_size\n",
    "        self.encoding_map = encoding_map\n",
    "        self.mps = mps\n",
    "        self.norm = norm #Normalize matrix data\n",
    "        #Compression mode \n",
    "        # \"Std\" standard Block Encoding\n",
    "        # \"DCT\" discrete cosine fourier transform before compression\n",
    "        self.mode = mode \n",
    "        self.min_value = min_value\n",
    "        self.max_value = max_value\n",
    "    \n",
    "    @classmethod\n",
    "    # @time_function\n",
    "    def from_matrix(cls, matrix, norm = False, mode = \"Std\"):\n",
    "        qubit_size, encoding_map = gen_encoding_map(matrix.shape)\n",
    "        encoding_map = np.moveaxis(encoding_map, 0, -1)\n",
    "\n",
    "        #check for flags\n",
    "        if norm:\n",
    "            matrix = matrix / (np.linalg.norm(matrix))\n",
    "        if mode == \"DCT\":\n",
    "            matrix = dct(matrix, norm = \"ortho\")\n",
    "\n",
    "        #initialize tensor\n",
    "        contracted_tensor = np.empty(shape = tuple(qubit_size))\n",
    "\n",
    "\n",
    "        #encode matrix data\n",
    "        # start_nested_loop = time.time()\n",
    "        it = np.nditer(matrix, flags=['multi_index'])\n",
    "        for _ in it:\n",
    "            contracted_tensor[tuple(encoding_map[it.multi_index])] = matrix[it.multi_index]\n",
    "        \n",
    "        # nested_loop_time = time.time() - start_nested_loop\n",
    "        # print(f\"Time for nested loops: {nested_loop_time:.4f} seconds\")\n",
    "        #put in MPS\n",
    "        # start_mps_creation = time.time()\n",
    "        mps = qtn.MatrixProductState.from_dense(contracted_tensor, dims = tuple(qubit_size))\n",
    "        # mps_creation_time = time.time() - start_mps_creation\n",
    "        # print(f\"Time to create MPS from dense tensor: {mps_creation_time:.4f} seconds\")\n",
    "        #return class\n",
    "        return cls(mps, qubit_size, encoding_map, norm, mode, np.min(matrix), np.max(matrix))\n",
    "\n",
    "    # @time_function\n",
    "    def compression_ratio(self):\n",
    "        initial_N = np.prod(self.qubit_size)\n",
    "        compressed_N = self.number_elements_in_MPS()\n",
    "        # TODO: also implement the compression rate in bits / bits\n",
    "        return compressed_N / initial_N\n",
    "        \n",
    "    # @time_function\n",
    "    def compress(self, cutoff):\n",
    "        \"\"\"\n",
    "        Compresses a Matrix Product State (MPS) by cutting bonds with a relative cutoff value.\n",
    "        Arguments:\n",
    "            cutoff (float): The relative cutoff value to use for bond compression.\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        size = len(self.mps.sites)\n",
    "        for i in np.arange(1, size):\n",
    "            t1 = self.mps[i-1] # Tensor 1\n",
    "            t2 = self.mps[i] # Tensor 2\n",
    "            # Compress bond according to percentage * bond dimension\n",
    "            qtn.tensor_compress_bond(t1, t2, cutoff = cutoff, cutoff_mode = \"rel\") \n",
    "    def continuous_compress(self, cutoff, print_ratio = True):\n",
    "        compress_list = np.array([0.01, 0.05, 0.1, 0.2, 0.5, 0.8, 1]) * cutoff\n",
    "        for c in compress_list:\n",
    "            self.compress(c)\n",
    "            if print_ratio:\n",
    "                print(f\"Compression ratio at {c}: {self.compression_ratio()}\")\n",
    "\n",
    "\n",
    "    # @time_function\n",
    "    def number_elements_in_MPS(self):\n",
    "        \"\"\"\n",
    "        Returns the number of tensor elements in the quimb MPS.\n",
    "        Parameters:\n",
    "            mps: quimb MatrixProductState object\n",
    "        Returns:\n",
    "            int: The total number of tensor elements in the MPS.\"\"\"\n",
    "        return sum(t.size for t in self.mps)\n",
    "    \n",
    "    # @time_function\n",
    "    def mps_to_matrix(self):\n",
    "        \"\"\"\n",
    "        Converts the compressed Matrix Product State (MPS) representation back to an image matrix.\n",
    "        Arguments:\n",
    "            None\n",
    "        Returns:\n",
    "            Compressed matrix\n",
    "        \"\"\"\n",
    "\n",
    "        #conract mps\n",
    "        contracted_mps = self.mps ^ ...\n",
    "\n",
    "        #order tensor legs back\n",
    "        for i in np.arange(len(contracted_mps.inds)):\n",
    "            contracted_mps.moveindex(\"k\"+str(i), i, inplace=True)\n",
    "        \n",
    "        #return in correct format\n",
    "\n",
    "        recovered_tensor = np.empty(self.encoding_map.shape)\n",
    "        it = np.nditer(recovered_tensor, flags=['multi_index'])\n",
    "        for _ in it:\n",
    "            recovered_tensor[it.multi_index] = contracted_mps.data[self.encoding_map[it.multi_index]]\n",
    "        \n",
    "        if self.mode == \"Std\":\n",
    "            return recovered_tensor\n",
    "        elif self.mode == \"DCT\":\n",
    "            return idct(recovered_tensor, norm = \"ortho\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_tens = np.random.rand(240,240,220)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2k/n_pr2lyj52ggrhmgv2zx_jt40000gn/T/ipykernel_36921/2243089732.py:39: RuntimeWarning: invalid value encountered in cast\n",
      "  prod_block_sizes[0] = prod_block_sizes[0] * 1e100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run hierarchical_block_indexing: 1.4560 seconds\n"
     ]
    }
   ],
   "source": [
    "mps_test = NDMPS_new.from_matrix(test_tens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maxge/Documents/Studium/München/02_SS 2024/QEL/Block encoding generalization/img-compression-mps/ND MPS Encoding/utils_ND.py:84: RuntimeWarning: invalid value encountered in cast\n",
      "  prod_block_sizes[0] = prod_block_sizes[0] * 1e100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time for nested loops: 1739022690.9270 seconds\n",
      "Time to create MPS from dense tensor: 2.4463 seconds\n"
     ]
    }
   ],
   "source": [
    "mps_old = NDMPS.from_matrix(test_tens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4223506.728076902"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mps_old.mps @ mps_test.mps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4223506.728076903"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mps_test.mps @ mps_test.mps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2k/n_pr2lyj52ggrhmgv2zx_jt40000gn/T/ipykernel_6455/2243089732.py:39: RuntimeWarning: invalid value encountered in cast\n",
      "  prod_block_sizes[0] = prod_block_sizes[0] * 1e100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run hierarchical_block_indexing: 1.3211 seconds\n",
      "(30, 32, 80, 165)\n"
     ]
    }
   ],
   "source": [
    "qubit_size, encoding_map = gen_encoding_map(test_tens.shape)\n",
    "encoding_map = np.moveaxis(encoding_map, 0, -1)\n",
    "\n",
    "contracted_tensor = np.empty(shape = tuple(qubit_size))\n",
    "print(contracted_tensor.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(240, 240, 220, 4)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding_map.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "shape_before = test_tens.shape\n",
    "k = encoding_map.shape[-1]\n",
    "flat_data = test_tens.flatten()\n",
    "flat_new_indices = encoding_map.reshape(-1, k).astype(int)\n",
    "new_shape = [flat_new_indices[:, dim].max() + 1 for dim in range(k)]\n",
    "new_tensor = np.zeros(new_shape, dtype=test_tens.dtype)\n",
    "indices = tuple(flat_new_indices[:, dim] for dim in range(k))\n",
    "new_tensor[indices] = flat_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "mps_try = qtn.MatrixProductState.from_dense(new_tensor, dims = tuple(qubit_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4223506.728076902"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mps_test.mps @ mps_try"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[np.int64(30), np.int64(32), np.int64(80), np.int64(165)]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "4223506.728076902"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NDMPS_speed:\n",
    "    def __init__(self, mps=None, qubit_size=None, encoding_map=None, norm=True, mode=\"Std\", min_value = 0, max_value = 1):\n",
    "        self.qubit_size = qubit_size\n",
    "        self.encoding_map = encoding_map\n",
    "        self.mps = mps\n",
    "        self.norm = norm #Normalize matrix data\n",
    "        #Compression mode \n",
    "        # \"Std\" standard Block Encoding\n",
    "        # \"DCT\" discrete cosine fourier transform before compression\n",
    "        self.mode = mode \n",
    "        self.min_value = min_value\n",
    "        self.max_value = max_value\n",
    "    \n",
    "    @classmethod\n",
    "    # @time_function\n",
    "    def from_matrix(cls, matrix, norm = False, mode = \"Std\"):\n",
    "        qubit_size, encoding_map = gen_encoding_map(matrix.shape)\n",
    "        encoding_map = np.moveaxis(encoding_map, 0, -1)\n",
    "\n",
    "        #check for flags\n",
    "        if norm:\n",
    "            matrix = matrix / (np.linalg.norm(matrix))\n",
    "        if mode == \"DCT\":\n",
    "            matrix = dct(matrix, norm = \"ortho\")\n",
    "\n",
    "        #initialize tensor\n",
    "        contracted_tensor = np.empty(shape = tuple(qubit_size), dtype=matrix.dtype)\n",
    "\n",
    "        # rearange the data\n",
    "        k = encoding_map.shape[-1]\n",
    "        flat_data = matrix.flatten()\n",
    "        flat_new_indices = encoding_map.reshape(-1, k).astype(int)\n",
    "        new_shape = [flat_new_indices[:, dim].max() + 1 for dim in range(k)]\n",
    "        indices = tuple(flat_new_indices[:, dim] for dim in range(k))\n",
    "        contracted_tensor[indices] = flat_data\n",
    "        \n",
    "        nested_loop_time = time.time()\n",
    "        print(f\"Time for nested loops: {nested_loop_time:.4f} seconds\")\n",
    "        #put in MPS\n",
    "        start_mps_creation = time.time()\n",
    "        mps = qtn.MatrixProductState.from_dense(contracted_tensor, dims = tuple(qubit_size))\n",
    "        mps_creation_time = time.time() - start_mps_creation\n",
    "        print(f\"Time to create MPS from dense tensor: {mps_creation_time:.4f} seconds\")\n",
    "        #return class\n",
    "        return cls(mps, qubit_size, encoding_map, norm, mode, np.min(matrix), np.max(matrix))\n",
    "\n",
    "    # @time_function\n",
    "    def compression_ratio(self):\n",
    "        initial_N = np.prod(self.qubit_size)\n",
    "        compressed_N = self.number_elements_in_MPS()\n",
    "        # TODO: also implement the compression rate in bits / bits\n",
    "        return compressed_N / initial_N\n",
    "        \n",
    "    # @time_function\n",
    "    def compress(self, cutoff):\n",
    "        \"\"\"\n",
    "        Compresses a Matrix Product State (MPS) by cutting bonds with a relative cutoff value.\n",
    "        Arguments:\n",
    "            cutoff (float): The relative cutoff value to use for bond compression.\n",
    "        Returns:\n",
    "            None\n",
    "        \"\"\"\n",
    "        size = len(self.mps.sites)\n",
    "        for i in np.arange(1, size):\n",
    "            t1 = self.mps[i-1] # Tensor 1\n",
    "            t2 = self.mps[i] # Tensor 2\n",
    "            # Compress bond according to percentage * bond dimension\n",
    "            qtn.tensor_compress_bond(t1, t2, cutoff = cutoff, cutoff_mode = \"rel\") \n",
    "    def continuous_compress(self, cutoff, print_ratio = True):\n",
    "        compress_list = np.array([0.01, 0.05, 0.1, 0.2, 0.5, 0.8, 1]) * cutoff\n",
    "        for c in compress_list:\n",
    "            self.compress(c)\n",
    "            if print_ratio:\n",
    "                print(f\"Compression ratio at {c}: {self.compression_ratio()}\")\n",
    "\n",
    "\n",
    "    # @time_function\n",
    "    def number_elements_in_MPS(self):\n",
    "        \"\"\"\n",
    "        Returns the number of tensor elements in the quimb MPS.\n",
    "        Parameters:\n",
    "            mps: quimb MatrixProductState object\n",
    "        Returns:\n",
    "            int: The total number of tensor elements in the MPS.\"\"\"\n",
    "        return sum(t.size for t in self.mps)\n",
    "    \n",
    "    # @time_function\n",
    "    def mps_to_matrix(self):\n",
    "        \"\"\"\n",
    "        Converts the compressed Matrix Product State (MPS) representation back to an image matrix.\n",
    "        Arguments:\n",
    "            None\n",
    "        Returns:\n",
    "            Compressed matrix\n",
    "        \"\"\"\n",
    "\n",
    "        #conract mps\n",
    "        contracted_mps = self.mps ^ ...\n",
    "\n",
    "        #order tensor legs back\n",
    "        for i in np.arange(len(contracted_mps.inds)):\n",
    "            contracted_mps.moveindex(\"k\"+str(i), i, inplace=True)\n",
    "        \n",
    "        #return in correct format\n",
    "        k = self.encoding_map.shape[-1]\n",
    "        \n",
    "        recovered_tensor = np.empty(self.encoding_map.shape)\n",
    "        contracted_mps = contracted_mps.data\n",
    "        recovered_tensor = contracted_mps[tuple(self.encoding_map[..., dim] for dim in range(k))]\n",
    "        \n",
    "        if self.mode == \"Std\":\n",
    "            return recovered_tensor\n",
    "        elif self.mode == \"DCT\":\n",
    "            return idct(recovered_tensor, norm = \"ortho\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/2k/n_pr2lyj52ggrhmgv2zx_jt40000gn/T/ipykernel_6455/2243089732.py:39: RuntimeWarning: invalid value encountered in cast\n",
      "  prod_block_sizes[0] = prod_block_sizes[0] * 1e100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time to run hierarchical_block_indexing: 1.2963 seconds\n",
      "Time for nested loops: 1737928377.1835 seconds\n",
      "Time to create MPS from dense tensor: 2.2617 seconds\n"
     ]
    }
   ],
   "source": [
    "mps_speed = NDMPS_speed.from_matrix(test_tens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4223506.728076902"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mps_speed.mps @ mps_test.mps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_tens = mps_speed.mps ^ ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(con_tens.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.43384858, 0.12321303, 0.92785341, ..., 0.90405203,\n",
       "         0.54961228, 0.07769605],\n",
       "        [0.56101817, 0.09163379, 0.05478229, ..., 0.1001284 ,\n",
       "         0.93329716, 0.18804416],\n",
       "        [0.17072938, 0.46400181, 0.76266317, ..., 0.35617674,\n",
       "         0.72476931, 0.93450828],\n",
       "        ...,\n",
       "        [0.6809151 , 0.00982142, 0.91035291, ..., 0.76871745,\n",
       "         0.76100618, 0.64403658],\n",
       "        [0.88386582, 0.57398938, 0.19550277, ..., 0.74408306,\n",
       "         0.40328463, 0.95067469],\n",
       "        [0.01815623, 0.07999099, 0.65302307, ..., 0.75318321,\n",
       "         0.29115235, 0.11161086]],\n",
       "\n",
       "       [[0.5642255 , 0.25830904, 0.56575468, ..., 0.30530762,\n",
       "         0.97005248, 0.27493029],\n",
       "        [0.38114399, 0.02276837, 0.5884885 , ..., 0.17433811,\n",
       "         0.01266093, 0.09198046],\n",
       "        [0.57487109, 0.01311029, 0.46535928, ..., 0.04451448,\n",
       "         0.91478219, 0.33848192],\n",
       "        ...,\n",
       "        [0.38537621, 0.4245431 , 0.39818488, ..., 0.58143511,\n",
       "         0.43378469, 0.28558417],\n",
       "        [0.06954298, 0.0497505 , 0.60524483, ..., 0.76428313,\n",
       "         0.369762  , 0.22593485],\n",
       "        [0.18129686, 0.88541581, 0.48320928, ..., 0.50684103,\n",
       "         0.4135507 , 0.80887228]],\n",
       "\n",
       "       [[0.94522747, 0.69365057, 0.88085524, ..., 0.06604056,\n",
       "         0.91540114, 0.56731196],\n",
       "        [0.41748798, 0.08352916, 0.16776738, ..., 0.91933113,\n",
       "         0.24140603, 0.22029421],\n",
       "        [0.54672503, 0.95540203, 0.83889459, ..., 0.57801691,\n",
       "         0.06510938, 0.22986034],\n",
       "        ...,\n",
       "        [0.19047143, 0.7289971 , 0.09097283, ..., 0.19630922,\n",
       "         0.66517773, 0.84379737],\n",
       "        [0.73461445, 0.36223359, 0.07849939, ..., 0.24615201,\n",
       "         0.06960945, 0.3288505 ],\n",
       "        [0.75481313, 0.7348201 , 0.17363809, ..., 0.93598477,\n",
       "         0.79274   , 0.64174357]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.01253056, 0.9615645 , 0.74507296, ..., 0.03548968,\n",
       "         0.471569  , 0.50574265],\n",
       "        [0.39114963, 0.25670416, 0.86488485, ..., 0.62521244,\n",
       "         0.50583574, 0.63006393],\n",
       "        [0.31125604, 0.81556971, 0.44794089, ..., 0.10905503,\n",
       "         0.86337   , 0.56497395],\n",
       "        ...,\n",
       "        [0.68653637, 0.58924647, 0.91851044, ..., 0.54987789,\n",
       "         0.00807477, 0.08683283],\n",
       "        [0.48239719, 0.74866693, 0.74420241, ..., 0.15645148,\n",
       "         0.29029566, 0.93178119],\n",
       "        [0.74368578, 0.099418  , 0.60452834, ..., 0.55692693,\n",
       "         0.56528054, 0.2438637 ]],\n",
       "\n",
       "       [[0.46384321, 0.97059163, 0.02860582, ..., 0.64656632,\n",
       "         0.29077126, 0.92459277],\n",
       "        [0.8048222 , 0.44812582, 0.01230451, ..., 0.33583363,\n",
       "         0.32824221, 0.30760051],\n",
       "        [0.3937707 , 0.091852  , 0.63570964, ..., 0.44055835,\n",
       "         0.81912104, 0.96053173],\n",
       "        ...,\n",
       "        [0.96447825, 0.13308696, 0.20625212, ..., 0.44900662,\n",
       "         0.26286375, 0.84761313],\n",
       "        [0.56267867, 0.07237924, 0.74770596, ..., 0.35279801,\n",
       "         0.25453239, 0.54060249],\n",
       "        [0.95412324, 0.96022544, 0.03483747, ..., 0.55591333,\n",
       "         0.51668881, 0.61027809]],\n",
       "\n",
       "       [[0.24938387, 0.74685474, 0.0475879 , ..., 0.41837318,\n",
       "         0.50088872, 0.00672065],\n",
       "        [0.82302617, 0.09632091, 0.77923005, ..., 0.17405992,\n",
       "         0.53885596, 0.97136264],\n",
       "        [0.1881582 , 0.97047611, 0.69901005, ..., 0.56721261,\n",
       "         0.94258199, 0.67131116],\n",
       "        ...,\n",
       "        [0.46711212, 0.08918047, 0.64347644, ..., 0.98638265,\n",
       "         0.09640484, 0.75952454],\n",
       "        [0.5672176 , 0.70053783, 0.7875673 , ..., 0.37448093,\n",
       "         0.67831869, 0.02039138],\n",
       "        [0.87516296, 0.46808457, 0.68488382, ..., 0.16620376,\n",
       "         0.10842814, 0.70970254]]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mps_speed.mps_to_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(9.409584222908052e-12)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(test_tens - mps_speed.mps_to_matrix())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Data:\n",
      " [[[ 0  1  2  3]\n",
      "  [ 4  5  6  7]\n",
      "  [ 8  9 10 11]]\n",
      "\n",
      " [[12 13 14 15]\n",
      "  [16 17 18 19]\n",
      "  [20 21 22 23]]]\n",
      "Encoding Map:\n",
      " [[[[0 0]\n",
      "   [1 0]\n",
      "   [2 0]\n",
      "   [0 1]]\n",
      "\n",
      "  [[1 1]\n",
      "   [2 1]\n",
      "   [0 2]\n",
      "   [1 2]]\n",
      "\n",
      "  [[2 2]\n",
      "   [0 3]\n",
      "   [1 3]\n",
      "   [2 3]]]\n",
      "\n",
      "\n",
      " [[[0 4]\n",
      "   [1 4]\n",
      "   [2 4]\n",
      "   [0 5]]\n",
      "\n",
      "  [[1 5]\n",
      "   [2 5]\n",
      "   [0 6]\n",
      "   [1 6]]\n",
      "\n",
      "  [[2 6]\n",
      "   [0 7]\n",
      "   [1 7]\n",
      "   [2 7]]]]\n",
      "New Tensor:\n",
      " [[ 0  3  6  9 12 15 18 21]\n",
      " [ 1  4  7 10 13 16 19 22]\n",
      " [ 2  5  8 11 14 17 20 23]]\n",
      "Reconstructed Data:\n",
      " [[[ 0  1  2  3]\n",
      "  [ 4  5  6  7]\n",
      "  [ 8  9 10 11]]\n",
      "\n",
      " [[12 13 14 15]\n",
      "  [16 17 18 19]\n",
      "  [20 21 22 23]]]\n",
      "Reconstruction successful!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example dimensions\n",
    "A, B, C = 2, 3, 4\n",
    "k = 2  # Number of new dimensions\n",
    "\n",
    "# Original data tensor (for demonstration)\n",
    "data = np.arange(A * B * C).reshape(A, B, C)\n",
    "print(\"Original Data:\\n\", data)\n",
    "\n",
    "# Encoding map tensor with new indices\n",
    "encoding_map = np.stack((data % 3, data // 3), axis=-1)  # Shape: (A, B, C, 2)\n",
    "print(\"Encoding Map:\\n\", encoding_map)\n",
    "\n",
    "# Create new_tensor based on encoding_map (simulating the forward mapping)\n",
    "new_shape = [encoding_map[..., dim].max() + 1 for dim in range(k)]\n",
    "new_tensor = np.zeros(new_shape, dtype=data.dtype)\n",
    "flat_data = data.flatten()\n",
    "flat_new_indices = encoding_map.reshape(-1, k)\n",
    "indices = tuple(flat_new_indices[:, dim] for dim in range(k))\n",
    "new_tensor[indices] = flat_data\n",
    "print(\"New Tensor:\\n\", new_tensor)\n",
    "\n",
    "# Reverse mapping: reconstruct the original data\n",
    "reconstructed_data = new_tensor[tuple(encoding_map[..., dim] for dim in range(k))]\n",
    "print(\"Reconstructed Data:\\n\", reconstructed_data)\n",
    "\n",
    "# Verify correctness\n",
    "assert np.array_equal(data, reconstructed_data), \"Reconstruction failed!\"\n",
    "print(\"Reconstruction successful!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstructed Data:\n",
      " [[[ 0  1  2  3]\n",
      "  [ 4  5  6  7]\n",
      "  [ 8  9 10 11]]\n",
      "\n",
      " [[12 13 14 15]\n",
      "  [16 17 18 19]\n",
      "  [20 21 22 23]]]\n"
     ]
    }
   ],
   "source": [
    "reconstructed_data = new_tensor[tuple(encoding_map[..., dim] for dim in range(k))]\n",
    "print(\"Reconstructed Data:\\n\", reconstructed_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 4)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstructed_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 3, 4, 2)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoding_map.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "QEL",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
